{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3839FCCET04g"
   },
   "source": [
    "# Reinforcement learning with Q-learning\n",
    "\n",
    "Reinforcement learning with Q-learning \n",
    "Reinforcement learning is a type of machine learning that requires the use of a different form of learning data. This type of learning is explicitly used for situations that require learning from environments. Have you ever wondered how a dog learns tricks? Let us consider how we could train a dog. \n",
    "The dog doesn't understand our language and needs to be taught how to do a certain trick. Since we can't tell the dog what to do we can follow a different strategy. We provide the dog with a prompt or a cue. For example, if we want the dog to sit we point at the floor and say 'Sit!'. At this point, the dog will respond to us with a response. Depending on the type of response, the dog will be provided with a reward. So, if the dog does nothing we don't reward it. If the dog moves around, we don't reward it. If it sits, only then do we reward it. The dog is learning what to do from positive experiences. \n",
    "Let us consider some key terms now: \n",
    "1. The agent here is the dog.\n",
    "2. The environment is us since we provide the result of the action.\n",
    "3. The action that takes the dog from one state to another is its action.\n",
    "4. The state is the state of the dog. For example: sitting, standing, walking.\n",
    "5. The reward is a value that the dog knows which equates to the number of snacks that it receives. \n",
    "\n",
    "\n",
    "We will now be looking into an example of reinforcement learning. Here is a game in which we need to pick up a passenger from one location and drop him/her off at another location. How do we do that? Let's import a few libraries and get started first. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y6JJ9bQrT04o"
   },
   "source": [
    "\n",
    "\n",
    "## 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gym\n",
      "  Downloading gym-0.26.2.tar.gz (721 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m721.7/721.7 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.18.0 in /home/codespace/.local/lib/python3.10/site-packages (from gym) (1.26.4)\n",
      "Collecting cloudpickle>=1.2.0 (from gym)\n",
      "  Downloading cloudpickle-3.0.0-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting gym-notices>=0.0.4 (from gym)\n",
      "  Downloading gym_notices-0.0.8-py3-none-any.whl.metadata (1.0 kB)\n",
      "Downloading cloudpickle-3.0.0-py3-none-any.whl (20 kB)\n",
      "Downloading gym_notices-0.0.8-py3-none-any.whl (3.0 kB)\n",
      "Building wheels for collected packages: gym\n",
      "  Building wheel for gym (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gym: filename=gym-0.26.2-py3-none-any.whl size=827617 sha256=16610cf7060dd7c5e49dea05e1f458208d88f418c058027bd88da8219c18a7fa\n",
      "  Stored in directory: /home/codespace/.cache/pip/wheels/b9/22/6d/3e7b32d98451b4cd9d12417052affbeeeea012955d437da1da\n",
      "Successfully built gym\n",
      "Installing collected packages: gym-notices, cloudpickle, gym\n",
      "Successfully installed cloudpickle-3.0.0 gym-0.26.2 gym-notices-0.0.8\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Collecting click\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.9/97.9 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: click\n",
      "Successfully installed click-8.1.7\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install gym\n",
    "# !pip install pygame\n",
    "!pip install click\n",
    "# !python3 -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "f2fReiw6T04p"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import pickle\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "\n",
    "import click\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TAcZpE8FT04p"
   },
   "source": [
    "We will be using the `gym.make()` function to make an environment and play the game. Run the code bellow and try it out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "LDj799q_T04q"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"Taxi-v3\")\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is an environment?\n",
    "\n",
    "OpenAI Gym is a toolkit for developing and comparing reinforcement learning algorithms. This is the gym open-source library, which gives you access to a standardized set of environments.\n",
    "\n",
    "Open AI Gym has an environment-agent arrangement. It simply means Gym gives you access to an “agent” which can perform specific actions in an “environment”. In return, it gets the observation and reward as a consequence of performing a particular action in the environment.\n",
    "\n",
    "This means that using the gym library we can create an \"environment\" in which we can have an agent and perform actions on it. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Class](./class.jpg)\n",
    "\n",
    "As an anology, consider yourself in the classroom alone. **In this case, you are the agent and the classroom is the environment. And if you choose to take a book, that is an action.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Red1Fq4wT04q"
   },
   "source": [
    "Let us learn a little bit about the gym environment that we are currently working with. Head over to [this](https://gym.openai.com/envs/Taxi-v2/) link to check out the source of the environment. We will try out a few functions with this environment and get started with setting it up. \n",
    "\n",
    "Can you find out the function which can reset the environment? Run it in the code block below. What is the output like? What does the output represent?\n",
    "\n",
    "Students to answer below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "jRKqZ8x1T04q",
    "outputId": "932171ca-c05a-4939-93e5-50f7282ab575"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Student Answer\n",
    "env.reset()\n",
    "\n",
    "# The number represents the state of the environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you remember the example of the student in the classroom? In that case if you, the student, goes to pick up a book that is counted as one action. \n",
    "\n",
    "In the code above, we have created an environment called 'Taxi-V3'. In this environment, we are trying to simulate a taxi. It looks something like this. \n",
    "\n",
    "![Taxi](./taxi.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what are the action that a taxi can take here? Make a guess.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i86BVHdwT04r"
   },
   "source": [
    "\n",
    "We can decide between 6 actions for out agent. \n",
    "\n",
    "0 = south  \n",
    "1 = north  \n",
    "2 = east  \n",
    "3 = west  \n",
    "4 = pickup  \n",
    "5 = dropoff  \n",
    "\n",
    "There is a function that allows us to perform a step in the environment. \n",
    "You can use the `env.step()` function to run an action. Try it out below. \n",
    "\n",
    "(Students to answer below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "prJd7kRxT04s",
    "outputId": "354b7c00-26f6-4f1b-bd7c-4d52ae941b03"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(184, -1, False, {'prob': 1.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# student answer\n",
    "#please enter the action eg 0:south:\n",
    "env.step(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1oLOTpruT04t"
   },
   "source": [
    "Attempt to use the `env.render()` function to render the environment to see it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "1wJDlogqT04u",
    "outputId": "d1380a55-ca73-4cd2-d595-9ef8649c6534"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :\u001b[34;1m\u001b[43mG\u001b[0m\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2KkzGok1T04u"
   },
   "source": [
    "### Try to create an instance of the game using a while loop. \n",
    "\n",
    "Let us try to make an instance of the game. For this we need to create a while loop first. Do you remember what a while loop is? Try to make it on your own first.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "done = False\n",
    "while not done:\n",
    "    # render environment\n",
    "    # get input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the function `env.render()` to render the environment and we can use `input()` to obtain our input. Try replacing that in the code above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "done = False\n",
    "while not done:\n",
    "    env.render() #Render the environment in this line\n",
    "    i = int(input())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we obtain an input what is the next step? It would be to refresh the environment with the action step being taken. In order to do that we need to run a step in the environment and then clear the output and re-render the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "done = False\n",
    "while not done:\n",
    "    env.render() #Render the environment in this line\n",
    "    i = int(input())\n",
    "    # run a step\n",
    "    # clear output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `clear_output(wait=True)` function here to clear the output and `obs,reward,complete,info = env.step(i)` to run a step. What are these variables we are obtaining from the step function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "done = False\n",
    "while not done:\n",
    "    env.render() #Render the environment in this line\n",
    "    i = int(input())\n",
    "    obs,reward,complete,info = env.step(i) # run a step on the environment here\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These variables are important. They tell us the state of the environment as it is. Obs gives us information about the position of the taxi and other pieces of the environment. Reward tells us if the action yielded a positive result or not. Complete tells us if the intended goal of picking up and dropping off a passenger has been met. Finally, info gives us miscellaneous data. now compile this and print out the variables.\n",
    "\n",
    "(Students to answer below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "BE0faYKdT04v",
    "outputId": "1802a453-eb51-4eb7-e71b-e0a6ddddf035"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation =  375 \n",
      "reward =  -1 \n",
      "done =  False \n",
      "information =  {'prob': 1.0}\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35m\u001b[34;1mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (West)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-117d30a4698c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Render the environment in this line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mclear_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcomplete\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# run a step on the environment here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openvino_notebooks/openvino_env/lib/python3.8/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openvino_notebooks/openvino_env/lib/python3.8/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "# student answer\n",
    "done = False\n",
    "while not done:\n",
    "    env.render() #Render the environment in this line\n",
    "    i = int(input())\n",
    "    clear_output(wait=True)\n",
    "    obs,reward,complete,info = env.step(i) # run a step on the environment here\n",
    "    print('Observation = ', obs, '\\nreward = ', reward, '\\ndone = ', complete, '\\ninformation = ', info)\n",
    "    done = complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Note- If the input disappers, re-run the cell. Still if the problem persists, shutdown the kernal and restart it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_yfB382NT04v"
   },
   "source": [
    "Now that we have worked with the environment and understand the problem. Let us define a few terms. \n",
    "\n",
    "**State** - The state is provided by the variable 'obs' in the code above. It defines the state of the environment.  \n",
    "**Agent** - Here is the taxi.  \n",
    "**Action** - The action is the variable that we are passing to the environment to perform. So the Agent performs the action.  \n",
    "**Reward** - The reward is a number that tells how well the player is doing. The fewer steps it takes to reach state of 'done' the better it is. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1lJMPTimT04w"
   },
   "source": [
    "## 2. Q-Learning\n",
    "\n",
    "Essentially, Q-learning lets the agent use the environment's rewards to learn, over time, the best action to take in a given state.\n",
    "\n",
    "In order to remember what has worked for the AI we store the result of each step in a table called the **Q-table**. This  table will have a map of (state, action) -> Q-value. The Q-value is a number that represents if an action is beneficial or not. \n",
    "\n",
    "Here is what our Q-table should look like\n",
    "\n",
    "![qlearning.png](./qlearning.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bTzdVo8UT04x"
   },
   "source": [
    "We need a few hyper parameters to be able to effectively implement the Q-learning algorithm. During the learning process we are able to modify the \n",
    "\n",
    "1. Alpha value. The Alpha value is any number between 0 and 1. It is a measure of the learning rate. \n",
    "2. Gamma value. This value is a measure of the how greedy our algorithm is. Having a gamma value of 0 makes our learning algorithm more short sighted. \n",
    "3. Epsilon value. This variable sets how much the training should rely on old data and how much it should rely on new data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at a few of these parameters more carefully. \n",
    "\n",
    "**Alpha**\n",
    "\n",
    "The alpha value represents how fast our model is learning. So if the learning rate is high, then the model would take 1 step to learn something but if the rate is low then the model would take more steps to learn. What does this mean for you?\n",
    "\n",
    "Learning rate matter greatly because a learning rate that is too low will take us too much time to learn. A learning rate that is too high wont give us an optimal result. So picking the right learning rate is important. We have provided hte apt. learning rate for this example. Often the leanring rate is a game of trial and error.\n",
    "\n",
    "![lr](./lr.png)\n",
    "\n",
    "**Gamma**\n",
    "\n",
    "The gamma value is crutial to decide how our model learns. if the gamma is too high then our model will be too far sighted and if gamma is too low it will be near sighted. Let us consider the example of a student studying for an exam. In order to prepare for an exam the student can be short sighted and study a random topic each day with focus. Or the student could be long sighted and make a plan but not study in a focused manner during the day. Im sure as students some of you might be able to relate to either of these examples. Some of us make a plan to study and prepare ourself with an agenda but fail to study at the moment. Whereas some of us study at the moment but lack long term vision and planning. Gamma value represents this conundrum. The key is to have an apt. gamma value like you need to focus on long term and short term targets. \n",
    "\n",
    "![lr](./lr1.jpg)\n",
    "\n",
    "**Epsilon**\n",
    "\n",
    "When we want oure model to learn more from past failures we can increase the epison value. What does this mean for our model? Some models benefit from past experiences more than others. And again, picking the right value is often a trial and error process. We have provided a value of 0.1 because we want to focus more on old learnings than new ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "AcqudRV4T04y"
   },
   "outputs": [],
   "source": [
    "\n",
    "# The hyperparameters\n",
    "alpha = 0.1\n",
    "gamma = 0.6\n",
    "epsilon = 0.1\n",
    "\n",
    "# More the no of epochs, better the learning. There is no one method to identify the no of epochs, it will depend on the model you are building.\n",
    "NUM_EPISODES = 50000 \n",
    "  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the number of episodes?\n",
    "\n",
    "One episode is one attempt at performing a successful taxi pick up and drop off. So within one episode we would loop through the actions until the model either fails or succeeds. \n",
    "\n",
    "Our next step is to make a q-table. Refer to the image of Q-table above and notice the axes. The x-axis on the table has 6 values and the y axis has 500. But we dont need to enter these manually. Look below to find the code to make a q-table. Note that np.zeros makes a table with all values to be zero. Take a look at the documentation [here](https://numpy.org/doc/stable/reference/generated/numpy.zeros.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ChwZuh5tT04y"
   },
   "source": [
    "#### Task: Solve the following function in python\n",
    "\n",
    "$$\n",
    "Q(state, action)  \\leftarrow (1 -  \\alpha ) *Q (state,action) +  \\alpha (reward +  \\gamma  \\max Q(next state, all  Actions))\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the first thing we need is a Q-value for a state,action. How do we obtain this? We can do this by simply refering to the Q-table as such `q_table[state, action]`. \n",
    "\n",
    "(Students to answer below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "OixtUGPZT048",
    "outputId": "63140459-b469-400f-d4d4-b0064b65bdda"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'state' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-dd08553e9140>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# student answer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnew_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mold_value\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnext_max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mq_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'state' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "old_value = q_table[state, action]\n",
    "next_max = np.max(q_table[next_state])\n",
    "\n",
    "# student answer\n",
    "new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "q_table[state, action] = new_value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oGjCD9S4T048"
   },
   "source": [
    "Let us try to train our model now. How can we start? Let us start once again with the basic loop first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, NUM_EPISODES+1):\n",
    "    # code here\n",
    "print(\"Training finished.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within this loop we must add another loop in which the aim would be to step wise iterate through the environment. We have done this before. Lets copy the code over. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, NUM_EPISODES+1):\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        \n",
    "        next_state, reward, done, info = env.step(action) # Performing the next step.\n",
    "        \n",
    "print(\"Training finished.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to obtain the `action` variable without which we cannot perform a step. Using the Q-table above how can we get the recommended action?\n",
    "\n",
    "The answer is to use `q_table[state]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, NUM_EPISODES+1):\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = np.argmax(q_table[state])\n",
    "        next_state, reward, done, info = env.step(action) # Performing the next step.\n",
    "        \n",
    "print(\"Training finished.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next step is to include the epsilon value. Remeber how there is a 10% chance to explore a new space? We can code it out as such."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, NUM_EPISODES+1):\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        if random.uniform(0, 1) < epsilon: # there is a 10% chance of exploring a new action rather than using previous knowledge\n",
    "            action = env.action_space.sample() # Explore action space\n",
    "        else:\n",
    "            action = np.argmax(q_table[state]) # Exploit learned values\n",
    "            \n",
    "        next_state, reward, done, info = env.step(action) # Performing the next step.\n",
    "        \n",
    "print(\"Training finished.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next step is to calculate and update the Q-table. How can we do this? Remember the formula we had used to find a new value of the q-table? Use the code from there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, NUM_EPISODES+1):\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        if random.uniform(0, 1) < epsilon: # there is a 10% chance of exploring a new action rather than using previous knowledge\n",
    "            action = env.action_space.sample() # Explore action space\n",
    "        else:\n",
    "            action = np.argmax(q_table[state]) # Exploit learned values\n",
    "            \n",
    "        next_state, reward, done, info = env.step(action) # Performing the next step.\n",
    "        \n",
    "        old_value = q_table[state, action]\n",
    "        next_max = np.max(q_table[next_state])\n",
    "        \n",
    "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "        q_table[state, action] = new_value\n",
    "        \n",
    "print(\"Training finished.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us add some more code to print results along the way and store some important data. \n",
    "\n",
    "(Students to answer below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "ynfvIeM6T048",
    "outputId": "5b5e8f9c-e574-49fa-9342-f4f074908199"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 50000\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "Training finished.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, NUM_EPISODES+1):\n",
    "    state = env.reset()\n",
    "\n",
    "    epochs, penalties, reward, = 0, 0, 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        if random.uniform(0, 1) < epsilon: # there is a 10% chance of exploring a new action rather than using previous knowledge\n",
    "            action = env.action_space.sample() # Explore action space\n",
    "        else:\n",
    "            action = np.argmax(q_table[state]) # Exploit learned values\n",
    "\n",
    "        next_state, reward, done, info = env.step(action) # Performing the next step.\n",
    "        \n",
    "        \n",
    "        old_value = q_table[state, action]\n",
    "        next_max = np.max(q_table[next_state])\n",
    "        \n",
    "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "        q_table[state, action] = new_value\n",
    "\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        state = next_state\n",
    "        epochs += 1\n",
    "  # to visualize the training  \n",
    "    if i<=5 :\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Episode: {i}\")\n",
    "        env.render()\n",
    "        \n",
    "    if i % 100 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Episode: {i}\")\n",
    "        env.render()    \n",
    "\n",
    "print(\"Training finished.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4-2kycfoT04-"
   },
   "source": [
    "Congratulations!!! You have successfully trained a Q-learning model. In the supervised and unsupervised learning models we saved models in the model objects, but what about this case? Can you answer, what is the model and how is it stored in this case?\n",
    "(Student answer below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JdTlarR-T04_"
   },
   "outputs": [],
   "source": [
    "# student answer\n",
    "# The model is stored in the form of a Q-table. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tAtbfP65T04_"
   },
   "source": [
    "## 3. Evaluation\n",
    "\n",
    "Let us now evaluate our Q-table. How can we do this? Well, we simply use the same training algorithm except we dont add the formula to update the Q-table. Try it yourself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "_qR3ABnAT04_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results after 100 episodes:\n",
      "Average timesteps per episode: 12.96\n",
      "Average penalties per episode: 0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "total_epochs, total_penalties = 0, 0\n",
    "episodes = 100\n",
    "\n",
    "for _ in range(episodes):\n",
    "    state = env.reset()\n",
    "    epochs, penalties, reward = 0, 0, 0\n",
    "    \n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = np.argmax(q_table[state])\n",
    "        state, reward, done, info = env.step(action)\n",
    "\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        epochs += 1\n",
    "\n",
    "    total_penalties += penalties\n",
    "    total_epochs += epochs\n",
    "\n",
    "print(f\"Results after {episodes} episodes:\")\n",
    "print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n",
    "print(f\"Average penalties per episode: {total_penalties / episodes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6mB2gXA0T05A"
   },
   "source": [
    "Congratulations on completing the evaluation. What do your evluation results point to? After this you should have easily understood the basics of reinforcement learning and how one can build a model for it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qf-xA3YqT05B"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <p style=\"color:brown;\">Recommended actions of the RL model on taxi Problem</p>\n",
    "\n",
    "Now let's use the trained RL model to make recommendation of actions for the taxi. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "done = False\n",
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "qf-xA3YqT05B"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You solved the Taxi puzzle!\n"
     ]
    }
   ],
   "source": [
    "while not done:\n",
    "    print(\"The model recommends:\", np.argmax(q_table[state]))\n",
    "    env.render()  \n",
    "    i = int(input())\n",
    "    clear_output(wait=True)\n",
    "    state,reward,complete,info = env.step(i)  \n",
    "    done = complete\n",
    "    \n",
    "print(\"You solved the Taxi puzzle!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qf-xA3YqT05B"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "(4) [Jupyter - Coach] Module 16.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
